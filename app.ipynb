{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c3af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# creating a Sales Developmenr Representative (SDR) ai agent using LangChain and Groq\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.tools import tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1447da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables:\n",
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\Kartik\\AppData\\Roaming\n",
      "APPLICATION_INSIGHTS_NO_STATSBEAT: true\n",
      "CHROME_CRASHPAD_PIPE_NAME: \\\\.\\pipe\\crashpad_32480_SDBSQEEFWFWVCUDY\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: DESKTOP-HGGGQ7A\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "CONDA_ALLOW_SOFTLINKS: false\n",
      "CONDA_DEFAULT_ENV: C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\n",
      "CONDA_EXE: C:\\Users\\Kartik\\anaconda3\\Scripts\\conda.exe\n",
      "CONDA_PREFIX: C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\n",
      "CONDA_PROMPT_MODIFIER: (C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv) \n",
      "CONDA_PYTHON_EXE: C:\\Users\\Kartik\\anaconda3\\python.exe\n",
      "CONDA_ROOT: C:\\Users\\Kartik\\anaconda3\n",
      "CONDA_SHLVL: 1\n",
      "CUDA_PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\n",
      "CUDA_PATH_V12_5: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\n",
      "DEBUG: WARN\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "EFC_14172_1592913036: 1\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "GROQ_API_KEY: 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\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\Kartik\n",
      "HUGGINGFACE_TOKEN: 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\n",
      "IGCCSVC_DB: AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAlXfOwv3zeEa7olY/1Tf22wQAAAACAAAAAAAQZgAAAAEAACAAAAC8TWgHE5p3O6MXIZmVy+h83mp62maXUiS2Km6Ee+ZsnAAAAAAOgAAAAAIAACAAAADLSCSGgu6vv2Zw0rv0ZOBMLzF8MIZU8XLKskhXNCVCAmAAAAADQiVqUU2OCoHIiFmkSnFmG9yaWVXMPm8HzQ/kWZRnsnU5BEA6+M75aqGejI34yGWWlRbtiNoqJCTpgXvd8Wau1h/gBSiD+uJDHzR61+EDT7vp5XYn1dGSDD36UOZywh1AAAAAT0cOEtKr2Qlp1FvRPSzzXV59gfjehGe6XX0y0Z94MgD5tbXm3NkB8CSLF7AdwTtJalbx8Ztw2AC0aYRMw3pMSw==\n",
      "JPY_INTERRUPT_EVENT: 2392\n",
      "LANGCHAIN_PROJECT: pr-new-hobby-13\n",
      "LANGSMTIH_TOKEN: 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\n",
      "LOCALAPPDATA: C:\\Users\\Kartik\\AppData\\Local\n",
      "LOGONSERVER: \\\\DESKTOP-HGGGQ7A\n",
      "NUMBER_OF_PROCESSORS: 16\n",
      "ONEDRIVE: C:\\Users\\Kartik\\OneDrive\n",
      "ONEDRIVECONSUMER: C:\\Users\\Kartik\\OneDrive\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP: undefined\n",
      "OS: Windows_NT\n",
      "PATH: c:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Library\\mingw-w64\\bin;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Library\\usr\\bin;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Library\\bin;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Scripts;C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\bin;C:\\Users\\Kartik\\anaconda3\\condabin;C:\\oraclexe\\app\\oracle\\product\\11.2.0\\server\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\app\\Kartik\\product\\21c\\dbhomeXE\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\nodejs;C:\\Program Files\\Git\\cmd;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.2.1;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\Kartik\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Kartik\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\MinGW\\bin;C:\\Users\\Kartik\\AppData\\Roaming\\npm;C:\\Program Files (x86)\\MongoDB Atlas CLI;.\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: 9a03\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PROMPT: (C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv) $P$G\n",
      "PSMODULEPATH: C:\\Users\\Kartik\\Documents\\WindowsPowerShell\\Modules;\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYTHONUNBUFFERED: 1\n",
      "PYTHONUTF8: 1\n",
      "PYTHON_FROZEN_MODULES: on\n",
      "SESSIONNAME: Console\n",
      "SSL_CERT_DIR: C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Library\\ssl\\certs\n",
      "SSL_CERT_FILE: C:\\Users\\Kartik\\OneDrive\\Desktop\\aiml\\venv\\Library\\ssl\\cacert.pem\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\Kartik\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\Kartik\\AppData\\Local\\Temp\n",
      "USERDOMAIN: DESKTOP-HGGGQ7A\n",
      "USERDOMAIN_ROAMINGPROFILE: DESKTOP-HGGGQ7A\n",
      "USERNAME: Kartik\n",
      "USERPROFILE: C:\\Users\\Kartik\n",
      "VBOX_MSI_INSTALL_PATH: C:\\Program Files\\Oracle\\VirtualBox\\\n",
      "VS140COMNTOOLS: C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools\\\n",
      "VSCODE_CODE_CACHE_PATH: C:\\Users\\Kartik\\AppData\\Roaming\\Code\\CachedData\\258e40fedc6cb8edf399a463ce3a9d32e7e1f6f3\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE: extensionHost\n",
      "VSCODE_CWD: C:\\Users\\Kartik\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "VSCODE_ESM_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_IPC_HOOK: \\\\.\\pipe\\a8e72f1a-1.100.3-main-sock\n",
      "VSCODE_L10N_BUNDLE_LOCATION: \n",
      "VSCODE_NLS_CONFIG: {\"userLocale\":\"en-us\",\"osLocale\":\"en-us\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"C:\\\\Users\\\\Kartik\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\resources\\\\app\\\\out\\\\nls.messages.json\",\"locale\":\"en-us\",\"availableLanguages\":{}}\n",
      "VSCODE_PID: 32480\n",
      "WINDIR: C:\\WINDOWS\n",
      "ZES_ENABLE_SYSMAN: 1\n",
      "_CONDA_OLD_CHCP: 437\n",
      "__CONDA_OPENSSL_CERT_DIR_SET: 1\n",
      "__CONDA_OPENSSL_CERT_FILE_SET: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### print the environment variables\n",
    "print(\"Environment Variables:\")\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e5774b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "os.environ[\"LANGSMTIH_TOKEN\"] = os.getenv(\"LANGSMTIH_TOKEN\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langsmith.ai\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-new-hobby-13\"\n",
    "os.environ[\"GROQ_API_KEY\"] = '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'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d0b824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcece931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceChangelogCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "659820df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langchain_docs', description='Useful for answering questions about LangSmith and LangChain.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000025145ADC5E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251E0061EB0>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000025145ADC7C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251E0061EB0>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"langchain_docs\",\n",
    "    description=\"Useful for answering questions about LangSmith and LangChain.\",\n",
    ")\n",
    "\n",
    "retriever_tool ### this is the tool we will use in the agent to answer questions about LangSmith and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16803822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.run(\"What is LangSmith?\") ### this is how we can use the tool to answer questions about LangSmith and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a20a01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lead management tool\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "LEADS_FILE = \"leads.json\"\n",
    "\n",
    "@tool\n",
    "def save_lead_info(info: str) -> str:\n",
    "    \"\"\"\n",
    "    Save structured lead info in the format:\n",
    "    \"Company: ABC Corp, Domain: Healthcare, Problem: Want better patient tracking, Budget: $10,000\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = {}\n",
    "        for line in info.split(\",\"):\n",
    "            key, val = line.strip().split(\":\", 1)\n",
    "            parsed[key.strip().lower()] = val.strip()\n",
    "        parsed[\"timestamp\"] = str(datetime.now())\n",
    "\n",
    "        with open(LEADS_FILE, \"a\") as f:\n",
    "            f.write(json.dumps(parsed) + \"\\n\")\n",
    "\n",
    "        return \"Lead information saved successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving lead info: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3721bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### media trigger tool for LangChain and LangSmith demos\n",
    "@tool\n",
    "def show_demo_media(context: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a product use-case or industry context, return a video or image demo link.\n",
    "    Example input: \"Example Demo\", \"Langsmith Dashboard\", \"LangSmith for AI Agents Video\"\n",
    "    \"\"\"\n",
    "    media_map = {\n",
    "        \"langsmith dashboard\": \"https://youtu.be/LEIghyTGgQk?feature=shared\",\n",
    "        \"langsmith for ai agents video\": \"https://youtu.be/gOK65vR0hIY?feature=shared\",\n",
    "        \"Example demo\": \"https://youtu.be/kYtnLaJeia8?feature=shared\",\n",
    "    }\n",
    "\n",
    "    for keyword, url in media_map.items():\n",
    "        if keyword in context.lower():\n",
    "            return f\"Here‚Äôs a quick demo for {keyword}: {url}\"\n",
    "\n",
    "    return \"Sorry, I couldn‚Äôt find a matching demo right now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea469e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, save_lead_info, show_demo_media] ### set of tools we will use in the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2b05ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Greet the user. Then ask for the following, one at a time: \n",
    "1. Company name \n",
    "2. Domain \n",
    "3. Problem statement \n",
    "4. Budget\n",
    "\n",
    "Once you have all four, summarize and call the save_lead_info tool.\n",
    "\n",
    "If they ask a product-related question, call the retriever_tool.\n",
    "If they ask for a demo, call the show_demo_media tool.\n",
    "If they ask anything other than these, respond witha friendly message acknowledging their question and offering to help with product info or demos.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b090708",
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting up a memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "### selecting the chat model\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1a0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### binding tools to the agent chat model\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bd226ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### agent executor\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(\n",
    "    model=model_with_tools,\n",
    "    prompt=prompt_template,\n",
    "    tools=tools,\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"sdr_agent\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94f792e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is LangSmith? Can you also show me a demo of your product?', additional_kwargs={}, response_metadata={}, id='2426e549-c0fd-43a3-8416-07ed8c390991'), AIMessage(content=\"Hello! I'm excited to help you. \\n\\nCan you please tell me what company you are from?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 2512, 'total_tokens': 2534, 'completion_time': 0.018333333, 'prompt_time': 0.313216752, 'queue_time': -0.373915563, 'total_time': 0.331550085}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_dadc9d6142', 'finish_reason': 'stop', 'logprobs': None}, id='run--9db5fff3-f924-4394-ad98-042e9db8faf2-0', usage_metadata={'input_tokens': 2512, 'output_tokens': 22, 'total_tokens': 2534})]\n"
     ]
    }
   ],
   "source": [
    "### try agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "response= agent_executor.invoke(\n",
    "   { \"messages\":[\n",
    "        HumanMessage(\n",
    "            content=\"What is LangSmith? Can you also show me a demo of your product?\"\n",
    "        )\n",
    "    ]}, config\n",
    ")\n",
    "print(response[\"messages\"]) ### this is the response from the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1aa7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xh3k', 'function': {'arguments': '{\"query\":\"Hello, I\\'d like to learn more about LangSmith and LangChain.\"}', 'name': 'langchain_docs'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 1246, 'total_tokens': 1352, 'completion_time': 0.088333333, 'prompt_time': 0.180333805, 'queue_time': 0.033526362999999976, 'total_time': 0.268667138}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_dadc9d6142', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b4e75c7a-6da9-4b1e-b137-7e028e4322bd-0', tool_calls=[{'name': 'langchain_docs', 'args': {'query': \"Hello, I'd like to learn more about LangSmith and LangChain.\"}, 'id': 'call_xh3k', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1246, 'output_tokens': 106, 'total_tokens': 1352})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content=\"Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\", name='langchain_docs', id='470c9551-a70e-40c6-8da6-fa2da62e6835', tool_call_id='call_xh3k')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"Hello! It's great to have you here. Can you please tell me a bit about your company? What's the company name?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 2545, 'total_tokens': 2573, 'completion_time': 0.023333333, 'prompt_time': 0.317677238, 'queue_time': -0.38973236899999997, 'total_time': 0.341010571}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run--0c2af83f-ca48-4575-9ed0-31ecfc465920-0', usage_metadata={'input_tokens': 2545, 'output_tokens': 28, 'total_tokens': 2573})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream (\n",
    "   { \"messages\":[\n",
    "        HumanMessage(\n",
    "            content=\"hello I need help with my lead management. Can you help me save a lead?\"\n",
    "        )\n",
    "    ]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "# print(response[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ea33177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", api_key=os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9419c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartik\\AppData\\Local\\Temp\\ipykernel_16640\\3353824168.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c247c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartik\\AppData\\Local\\Temp\\ipykernel_16640\\3504736060.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor2 = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "agent_executor2 = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d727af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartik\\AppData\\Local\\Temp\\ipykernel_16640\\2672174587.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(agent_executor2.run(\"hello I need help with my lead management. Can you help me save a lead?\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: save_lead_info\n",
      "Action Input: \"Company: XYZ Inc, Domain: Technology, Problem: Want to automate sales process, Budget: $5,000\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError saving lead info: not enough values to unpack (expected 2, got 1)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: save_lead_info\n",
      "Action Input: \"Company: XYZ Inc, Domain: Technology, Problem: Want to automate sales process, Budget: $5,000\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError saving lead info: not enough values to unpack (expected 2, got 1)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: It looks like there's a problem saving the lead info. Can you provide more details about the lead, such as the company name, domain, problem they're trying to solve, and budget? This will help me assist you better. What's the issue you're experiencing with automating the sales process?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "It looks like there's a problem saving the lead info. Can you provide more details about the lead, such as the company name, domain, problem they're trying to solve, and budget? This will help me assist you better. What's the issue you're experiencing with automating the sales process?\n"
     ]
    }
   ],
   "source": [
    "print(agent_executor2.run(\"hello I need help with my lead management. Can you help me save a lead?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ec4afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are Ava, a friendly SDR AI voice assistant.\n",
    "Your job is to greet the user, ask for their company name, domain, problem, and budget.\n",
    "Answer product questions using the retriever_tool.\n",
    "If they request a demo, use the show_demo_media tool.\n",
    "If you use the show_demo_media tool, provide a media (vieo/image) link within the text so that user can access the link.\n",
    "Once all information is gathered, use save_lead_info and end the conversation politely.\n",
    "\"\"\"\n",
    "\n",
    "# Use the llm variable already defined, and pass your system_template as part of the prompt if needed.\n",
    "# The initialize_agent function does not take a system_message argument directly.\n",
    "# If you want to customize the system prompt, use the prompt argument in initialize_agent or PromptTemplate.\n",
    "\n",
    "agent_executor2 = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    prompt=prompt_template,  # Use the prompt_template defined earlier\n",
    "        # You can add a custom prompt here if needed, e.g.:\n",
    "    # prompt=PromptTemplate(input_variables=[], template=system_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3841a05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: langchain_docs\n",
      "Action Input: \"LangSmith\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: show_demo_media\n",
      "Action Input: \"LangSmith Dashboard\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mHere‚Äôs a quick demo for langsmith dashboard: https://youtu.be/LEIghyTGgQk?feature=shared\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: It's great that you're interested in LangSmith. As I mentioned earlier, it's a platform for building production-grade LLM applications. The key benefits of LangSmith include its observability features, which allow you to closely monitor and evaluate your application. Additionally, it offers evals, which enable you to evaluate your application over production traffic and get human feedback on your data. The prompt engineering feature is also quite useful, as it allows you to iterate on prompts with automatic version control and collaboration features. I'd be happy to help you explore LangSmith further or assist you with any specific questions you may have about it. Now, let's get back to your original question about saving a lead. Can you provide more details about the lead, such as the company name, domain, problem they're trying to solve, and budget? This will help me assist you better.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "It's great that you're interested in LangSmith. As I mentioned earlier, it's a platform for building production-grade LLM applications. The key benefits of LangSmith include its observability features, which allow you to closely monitor and evaluate your application. Additionally, it offers evals, which enable you to evaluate your application over production traffic and get human feedback on your data. The prompt engineering feature is also quite useful, as it allows you to iterate on prompts with automatic version control and collaboration features. I'd be happy to help you explore LangSmith further or assist you with any specific questions you may have about it. Now, let's get back to your original question about saving a lead. Can you provide more details about the lead, such as the company name, domain, problem they're trying to solve, and budget? This will help me assist you better.\n"
     ]
    }
   ],
   "source": [
    "# print(agent_executor2.run(\"hello I need help with my lead management. Can you help me save a lead?\"))\n",
    "print(agent_executor2.run(\"What is LangSmith? Can you also show me a demo of your product?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2be78711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: save_lead_info\n",
      "Action Input: \"Company: ABC Corp, Domain: Healthcare, Problem: Want better patient tracking, Budget: $10,000\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError saving lead info: not enough values to unpack (expected 2, got 1)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: save_lead_info\n",
      "Action Input: \"Company: ABC Corp, Domain: Healthcare, Problem: Want better patient tracking, Budget: $10,000\"\n",
      "Action: langchain_docs\n",
      "Action Input: lead info\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLead information saved successfully!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Based on the lead information you provided, it seems like ABC Corp is looking to improve patient tracking in the healthcare industry. I'd be happy to help you explore ways to achieve this goal.\n",
      "\n",
      "Given your budget of $10,000, we can consider various solutions that might be suitable for your company. One option could be using LangSmith's Observability feature, which can help you monitor and track patient information more effectively.\n",
      "\n",
      "Additionally, I'd like to ask some follow-up questions to better understand your needs. Could you please provide more information about your current patient tracking system and what specific pain points you're experiencing? This will help me provide more tailored suggestions and ideas for improvement.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the lead information you provided, it seems like ABC Corp is looking to improve patient tracking in the healthcare industry. I'd be happy to help you explore ways to achieve this goal.\n",
      "\n",
      "Given your budget of $10,000, we can consider various solutions that might be suitable for your company. One option could be using LangSmith's Observability feature, which can help you monitor and track patient information more effectively.\n",
      "\n",
      "Additionally, I'd like to ask some follow-up questions to better understand your needs. Could you please provide more information about your current patient tracking system and what specific pain points you're experiencing? This will help me provide more tailored suggestions and ideas for improvement.\n"
     ]
    }
   ],
   "source": [
    "print(agent_executor2.run(\"I own a healthcare company called ABC Corp. We want to improve patient tracking. Our budget is $10,000.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0538a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: show_demo_media\n",
      "Action Input: \"Langsmith Dashboard\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mHere‚Äôs a quick demo for langsmith dashboard: https://youtu.be/LEIghyTGgQk?feature=shared\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: I've shown you a demo of the Langsmith Dashboard. As I mentioned earlier, LangSmith is a platform for building production-grade LLM applications. It offers features such as observability, evals, and prompt engineering. Let me know if you have any further questions or if there's anything else I can help you with. If you'd like to see more demos or explore specific features of LangSmith, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I've shown you a demo of the Langsmith Dashboard. As I mentioned earlier, LangSmith is a platform for building production-grade LLM applications. It offers features such as observability, evals, and prompt engineering. Let me know if you have any further questions or if there's anything else I can help you with. If you'd like to see more demos or explore specific features of LangSmith, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# print(agent_executor2.run(\"What is my lead information you stored?\"))\n",
    "print(agent_executor2.run(\"Can you show me a demo of your product?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
